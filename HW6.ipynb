{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21405ba",
   "metadata": {},
   "source": [
    "1 The Simple Linear Regression model is a method used to describe the relationship between two variables: a predictor (or independent variable) ð‘‹ and an outcome (or dependent variable) ð‘Œ This relationship is expressed by the equation:Y=Î²_0+Î²_1X+Ïµ\n",
    "1) The x is predictor variable which predict the outcome and it is independent variable\n",
    "2) The y is the outcome variable we trying to predict.Itâ€™s the dependent variable\n",
    "3) The Î²_1 is the slope coefficient This coefficient represents the rate of change in ð‘Œ with respect to ð‘‹\n",
    "4) The Î²_0 is is the value of ð‘Œ when ð‘‹ = 0 It represents the point where the regression line crosses the Y-axis\n",
    "5) Error Term (Ïµ): This term represents the randomness or unexplained variation in Y that the model doesn't capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample data\n",
    "x = X  # predictor variable\n",
    "y = Y  # outcome variable\n",
    "\n",
    "# Create the figure and add the scatter plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data'))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Scatter plot of Y vs. X without trendline\",\n",
    "    xaxis_title=\"Predictor (X)\",\n",
    "    yaxis_title=\"Outcome (Y)\"\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a898cd",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/67202f06-8100-8007-83e7-45531a4ef9ff\n",
    "Hereâ€™s a summary of the key information from your questions:\n",
    "\n",
    "1. Simple Linear Regression Model:\n",
    "   - The model predicts an outcome (\\(Y\\)) based on a predictor (\\(X\\)) using a slope (\\(\\beta_1\\)), intercept (\\(\\beta_0\\)), and an error term (\\(\\epsilon\\)).\n",
    "   - The formula \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) captures this relationship, where \\(\\epsilon\\) accounts for the unexplained variability around the regression line.\n",
    "   - Python code using `numpy` and `scipy.stats` can simulate this model and show how observed values distribute around the line.\n",
    "\n",
    "2. Plotting Data in `plotly`:\n",
    "   - `plotly.express` with `trendline='ols'`: Automatically adds a fitted regression line, suitable for visualizing trends when modeling the data.\n",
    "   - `plotly.graph_objects` without trendline: Preferred for purely visualizing raw data distribution without fitting a trend line, providing greater control over the plotâ€™s content.\n",
    "   - For situations where the focus is on the raw data, `plotly.graph_objects` is recommended, as it avoids implying a fitted model.\n",
    "\n",
    "This ensures clarity in regression visualization and data plotting, depending on whether a trendline is desired or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f35206",
   "metadata": {},
   "source": [
    "2 Step 1: Import Libraries and Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf  # This library is used for specifying and fitting statistical models, especially linear regression models.\n",
    "import plotly.express as px  # This is a plotting library that makes it easy to create interactive visualizations.\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.random.rand(n) * 10\n",
    "epsilon = np.random.normal(0, 1, n)\n",
    "y = 3 + 2 * x + epsilon  # Theoretical model: Y = 3 + 2x + error\n",
    "\n",
    "# Combine into DataFrame\n",
    "df = pd.DataFrame({'x': x, 'Y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29effb",
   "metadata": {},
   "source": [
    "Step 2: Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f62650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the following two steps doing?\n",
    "model_data_specification = smf.ols(\"Y ~ x\", data=df)  \n",
    "# This step defines the regression model with Y as the dependent variable and x as the predictor.\n",
    "\n",
    "fitted_model = model_data_specification.fit()\n",
    "# This step fits the model to the data, calculating the parameters (e.g., slope and intercept).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133abad",
   "metadata": {},
   "source": [
    "Step 3: Model Summary Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c25648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model outputs\n",
    "fitted_model.summary()  \n",
    "# This output provides a comprehensive summary of the regression analysis, including details like coefficients, standard errors, R-squared, and p-values.\n",
    "\n",
    "fitted_model.summary().tables[1]\n",
    "# This outputs the table of regression coefficients specifically, which shows the values for the intercept and the slope, including their standard errors and significance levels.\n",
    "\n",
    "fitted_model.params\n",
    "# This provides the fitted regression parameters (coefficients), which includes the intercept and the slope.\n",
    "\n",
    "fitted_model.params.values  \n",
    "# This gives the parameter values as a simple array, without labels.\n",
    "\n",
    "fitted_model.rsquared  \n",
    "# This provides the R-squared value, which indicates the proportion of variance in Y explained by the predictor x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe062d",
   "metadata": {},
   "source": [
    "Step 4: Visualize with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data label for legend\n",
    "df['Data'] = 'Data'  # This step labels all points in the dataset for the legend.\n",
    "\n",
    "# Scatter plot with trendline\n",
    "fig = px.scatter(df, x='x', y='Y', color='Data', trendline='ols', title='Y vs. x')\n",
    "# Here, `trendline='ols'` adds the Ordinary Least Squares trendline to the scatter plot.\n",
    "\n",
    "# This essentially recreates what trendline='ols' does\n",
    "fig.add_scatter(x=df['x'], y=fitted_model.fittedvalues, line=dict(color='blue'), name=\"trendline='ols'\")\n",
    "\n",
    "fig.show(renderer=\"png\")  # Use renderer=\"png\" for compatibility with GitHub and MarkUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a3fb0",
   "metadata": {},
   "source": [
    "3  \n",
    "Theoretical Line: The theoretical line represents an idealized relationship between x and y, based on specific, fixed parameters (ð›½0 and ð›½1). It assumes no random sampling variation and reflects only the expected relationship, without incorporating any noise or error from real data. This line is fixed and unchanging across multiple simulations, serving as a constant reference for comparison, regardless of how many times new data samples are generated.\n",
    "\n",
    "In contrast, the fitted line, generated through Ordinary Least Squares (OLS) regression, adjusts to observed data points and accounts for the variability inherent in each dataset. Due to random sampling variation, this line may differ slightly from the theoretical line with each new sample. And the fitted line changes depending on the specific data points sampled, while the theoretical line remains constant.\n",
    "\n",
    "Nature of the Difference: This comparison highlights how sample data introduces deviations from theoretical expectations. The theoretical line provides a benchmark, while the OLS-fitted line reflects real-world data trends, showing the impact of sampling variation on model fitting. By repeatedly generating and plotting different samples, one would observe that the fitted line shifts with each sample, while the theoretical line remains consistent. This process emphasizes how random sampling affects the estimated model compared to the idealized theoretical model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Simulate data for Simple Linear Regression (from Question 2)\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "beta0 = 5\n",
    "beta1 = 2\n",
    "sigma = 1\n",
    "x = np.random.uniform(0, 10, n)\n",
    "errors = np.random.normal(0, sigma, n)\n",
    "y_simulated = beta0 + beta1 * x + errors  # Simulated Y values based on theoretical model\n",
    "\n",
    "# Combine simulated data into a DataFrame for plotting\n",
    "df_simulated = pd.DataFrame({'x': x, 'Y_simulated': y_simulated})\n",
    "\n",
    "# Create the observed data plot (Question 1)\n",
    "df_observed = pd.DataFrame({'x': x, 'Y': y_simulated + np.random.normal(0, 0.5, n), 'Data': 'Observed Data'})  # add small variation\n",
    "\n",
    "# Fit a regression model on observed data\n",
    "x_with_constant = sm.add_constant(df_observed['x'])\n",
    "model = sm.OLS(df_observed['Y'], x_with_constant)\n",
    "results = model.fit()\n",
    "\n",
    "# Plot observed data and trendline\n",
    "fig = px.scatter(df_observed, x='x', y='Y', color='Data', trendline='ols', title='Observed Y vs. x with Simulated Line')\n",
    "\n",
    "# Add simulated line from Question 2\n",
    "fig.add_scatter(\n",
    "    x=df_simulated['x'],\n",
    "    y=df_simulated['Y_simulated'],\n",
    "    mode='lines',\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    name=\"Simulated line (y = Î²0 + Î²1x)\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# what does this add onto the figure in constrast to `trendline='ols'`?\n",
    "x_range = np.array([df['x'].min(), df['x'].max()])\n",
    "# beta0 and beta1 are assumed to be defined\n",
    "y_line = beta0 + beta1 * x_range\n",
    "fig.add_scatter(x=x_range, y=y_line, mode='lines',\n",
    "                name=str(beta0)+' + '+str(beta1)+' * x', \n",
    "                line=dict(dash='dot', color='orange'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f902f7",
   "metadata": {},
   "source": [
    "chatbot\n",
    "Hereâ€™s a summary of all the questions and answers from the exercise:\n",
    "\n",
    "1. Purpose of Libraries:\n",
    "    `statsmodels.formula.api`: Used for specifying and fitting statistical models in Python, enabling formula-based regression analysis.\n",
    "    `plotly.express`: A high-level library for quick, interactive visualizations.\n",
    "\n",
    "2. Regression Model Specification and Fitting:\n",
    "    Code `model_data_specification = smf.ols(\"Y~x\", data=df)` specifies an Ordinary Least Squares (OLS) regression model with `Y` as the dependent variable and `x` as the independent variable.\n",
    "    `fitted_model = model_data_specification.fit()` fits this model to the data.\n",
    "\n",
    "3. Interpretation of Model Outputs:\n",
    "    `fitted_model.summary()`: Detailed model summary, including statistics like R-squared and coefficients.\n",
    "    `fitted_model.summary().tables[1]`: Contains main coefficient estimates and related statistics.\n",
    "    `fitted_model.params`: The estimated model parameters (intercept and slope).\n",
    "    `fitted_model.params.values`: The parameter values without labels.\n",
    "    `fitted_model.rsquared`: The R-squared value, indicating model fit quality.\n",
    "\n",
    "4. Adding Data Points and Trendline to the Plot:\n",
    "    A new column in the data adds a label for the legend.\n",
    "    `px.scatter` creates a scatter plot with an OLS trendline for visualizing the observed relationship between `x` and `Y`.\n",
    "\n",
    "5. Explanation of the Theoretical Simple Linear Regression Model:\n",
    "    The Simple Linear Regression model is represented as \\( Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\), where:\n",
    "     \\( \\beta_0 \\) is the intercept, and \\( \\beta_1 \\) is the slope.\n",
    "     \\( \\epsilon_i \\) represents random error, assumed normally distributed.\n",
    "     This model seeks to estimate \\( \\beta_0 \\) and \\( \\beta_1 \\) to predict `Y` from `X`.\n",
    "\n",
    "6. Python Code for Simulating Data and Fitting a Model:\n",
    "     Python code simulates data for a Simple Linear Regression and fits an OLS model using `statsmodels`.\n",
    "\n",
    "7. Adding Theoretical Line to Observed Data Plot:\n",
    "    Theoretical line is added based on fixed parameters (`beta0` and `beta1`) using `fig.add_scatter`.\n",
    "    This line (orange dashed) is compared with the observed trendline (`trendline='ols'`) to distinguish between the idealized relationship and the one influenced by sampling variation.\n",
    "\n",
    "8. Difference Between Observed Trendline and Theoretical Line:\n",
    "    The observed trendline reflects sampling variation, so it changes with different datasets.\n",
    "    The theoretical line is fixed based on predetermined parameters and does not vary across simulations, showing the \"true\" relationship without additional noise.\n",
    "\n",
    "This summarizes the overall tasks, purpose, and explanations related to Simple Linear Regression, visualization, and the role of simulation in understanding model variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290904ed",
   "metadata": {},
   "source": [
    "4  \n",
    "The fitted_model.fittedvalues in a statistical model represent the predicted values for the dependent variable based on the fitted regression equation. These values are calculated directly using the coefficients in fitted_model.params (or fitted_model.params.values) Then predict for each observation, resulting in the predicted values for the dependent variable.\n",
    "detailed steps:\n",
    "1 Fitted Regression Equatio\n",
    "2 Deriving fitted_model.fittedvalues: Each fitted value in fitted_model.fittedvalues is calculated by plugging the observed values of the predictors for each observation into the fitted equation.\n",
    "3 Link to fitted_model.summary().tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac4c19",
   "metadata": {},
   "source": [
    "chatbot\n",
    "In a Simple Linear Regression model, in-sample predictions (fitted values) are calculated by applying the regression equation to each observation in the dataset. The equation is\n",
    "  hat{y}_i = Î²0 + Î²1Xi\n",
    "where:\n",
    " (\\hat{y}_i\\) is the predicted (fitted) value for observation (i\\),\n",
    " (\\beta_0\\) is the intercept (constant term),\n",
    " (\\beta_1\\) is the slope (coefficient for \\(x\\)),\n",
    " (x_i\\) is the predictor value for observation \\(i\\).\n",
    "\n",
    "Using `fitted_model.params` (where `fitted_model.params[0]` is the intercept and `fitted_model.params[1]` is the slope), the model computes fitted values by plugging each observed \\(x_i\\) into this equation.\n",
    "\n",
    "For example, if \\(\\beta_0 = 2.5\\), \\(\\beta_1 = 0.8\\), and predictor values are \\(X = [1, 2, 3, 4]\\):\n",
    "The fitted values are \\([3.3, 4.1, 4.9, 5.7]\\), calculated by substituting each \\(x_i\\) into the equation.\n",
    "\n",
    "Thus, `fitted_model.fittedvalues` are derived by applying the estimated coefficients from `fitted_model.params` to each observation's predictor values, yielding in-sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f589b6",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6722717c-8148-8007-9142-a5c131a1c1ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34bf05c",
   "metadata": {},
   "source": [
    "5.1  In an \"ordinary least squares\" (OLS) model, the fitted line is chosen to minimize the sum of the squared differences (errors) between observed data points and the predicted values on the line. The line is \"fitted\" by finding the slope and intercept that result in the smallest possible total of these squared errors.\n",
    "\n",
    "5.2  Using squares rather than simple absolute differences is key because squaring emphasizes larger errors, making the model more sensitive to outliers and ensuring that positive and negative errors don't cancel each other out. This approach produces a single, optimal line that represents the best linear fit for the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bf591",
   "metadata": {},
   "source": [
    "chatbot Here's a summary of your questions:\n",
    "\n",
    "1 What line is chosen in the OLS method, and why are \"squares\" used?  \n",
    "    The OLS method chooses the line that minimizes the sum of squared residuals (differences between observed and predicted values) to best fit the data. Squaring emphasizes larger errors, prevents cancellation of positive and negative differences, and allows for an efficient solution.\n",
    "\n",
    "2 What is the purpose of the OLS method? \n",
    "   The purpose of OLS is to find a line that best represents the relationship between variables by minimizing prediction errors. This line reveals trends, provides a simple model for interpretation, enables statistical inference, and supports future predictions.\n",
    "\n",
    "In summary, the OLS method aims to find a trendline that accurately reflects observed data and minimizes error, offering insights and predictive capabilities for analysis and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063aad1",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/672286f5-271c-8007-a103-0e4cc7771e40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271dfc56",
   "metadata": {},
   "source": [
    "6 \n",
    "6.1For the first expression:\n",
    "The expression 1 - ((Y - fitted_model.fittedvalues)**2).sum() / ((Y - Y.mean())**2).sum() calculates the proportion\n",
    "calculates the proportion of the variation in Y that is explained by the model. This is actually equivalent to \n",
    "ð‘…^2, a measure of how well the model's fitted values match the actual values of Y. Hereâ€™s why:\n",
    "The numerator, ((Y - fitted_model.fittedvalues)**2).sum(), represents the sum of squared residuals (SSR), \n",
    "showing the unexplained variation.\n",
    "The denominator, ((Y - Y.mean())**2).sum(), is the total sum of squares (TSS), which measures the total variation in Y.Thus, the entire expression represents 1 - (SSR/TSS), which gives the proportion of the variation in Y explained by the model.\n",
    "\n",
    "6.2 fitted_model.rsquared:\n",
    "This is the direct R^2 value, which is a standardized measure of the model's goodness of fit. It quantifies the accuracy of the model by showing how much of the variability in Y is accounted for by the linear relationship with the predictors.\n",
    "\n",
    "6.3 np.corrcoef(Y, fitted_model.fittedvalues)[0,1]:\n",
    "This represents the correlation between Y and the modelâ€™s fitted values. Squaring this correlation coefficient gives a value equivalent to R^2 in the case of simple linear regression, since R^2 is the square of the correlation between the observed and predicted values.\n",
    "\n",
    "6.4 np.corrcoef(Y, x)[0,1]:\n",
    "This calculates the correlation between the outcome Y and the predictor x. Squaring this correlation also gives the R^2 value in simple linear regression, as there's only one predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e937e",
   "metadata": {},
   "source": [
    "chatbot: https://chatgpt.com/share/6722912a-451c-8007-a7b3-b0ef8d6807ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5e5cc",
   "metadata": {},
   "source": [
    "7\n",
    "In Simple Linear Regression (SLR), several key assumptions are needed for the model to be valid and yield reliable results.\n",
    "Linearity: The relationship between the independent variable X and the dependent variable \n",
    "Y must be linear. If the scatter plot of the data shows a curved or non-linear trend, this assumption is violated, indicating that SLR might not be appropriate for modeling this relationship.\n",
    "\n",
    "Homoscedasticity (Constant Variance of Errors): The variance of residuals (the differences between observed and predicted values) should be constant across all values of ð‘‹ If the variance changes (e.g., residuals become larger as ð‘‹ increases), this assumption is violated. This could show up in a residual plot as a \"funnel\" shape or other patterns, suggesting that the model's predictions may become less reliable as ð‘‹ changes.\n",
    "\n",
    "Normality of Residuals: The histogram of residuals (Histogram of Residuals) helps assess whether residuals are approximately normally distributed. If the residuals show skewness, significant peaks, or irregular patterns, the assumption of normality is violated. Non-normal residuals may indicate issues like outliers or a non-linear relationship, affecting the reliability of inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774e567",
   "metadata": {},
   "source": [
    "8 \n",
    "\n",
    "To specify the null hypothesis and characterize the evidence,these part of the requirement for this linear regression model.\n",
    "\n",
    "Step 1: Specify the Null Hypothesis in Terms of the Parameter\n",
    "In the context of a Simple Linear Regression model, we are investigating whether there is a linear association between the **waiting time** (predictor) and **eruption duration** (response) in the Old Faithful Geyser dataset.\n",
    "\n",
    "The regression equation is:\n",
    "\\[\n",
    "\\text{duration} = \\beta_0 + \\beta_1 \\times \\text{waiting} + \\epsilon\n",
    "\\]\n",
    "where:\n",
    "\\(\\beta_0\\) is the intercept.\n",
    "\\(\\beta_1\\) is the slope (coefficient for `waiting`).\n",
    "\\(\\epsilon\\) is the error term.\n",
    "\n",
    "Null Hypothesis (Hâ‚€): There is no linear association between waiting time and eruption duration. In terms of the parameter, this means:\n",
    "\\[\n",
    "H_0: \\beta_1 = 0\n",
    "\\]\n",
    "\n",
    "If \\(\\beta_1 = 0\\), it implies that changes in waiting time do not have a statistically significant effect on eruption duration.\n",
    "\n",
    " Step 2: Run the Model and Interpret the p-value\n",
    "The code provided fits a linear regression model, then outputs a summary that includes the p-value for \\(\\beta_1\\). Hereâ€™s a breakdown of what each outcome would mean based on the p-value:\n",
    "\n",
    "If p > 0.1: We have no evidence against the null hypothesis; waiting time likely has no effect on eruption duration.\n",
    "If 0.1 â‰¥ p > 0.05: We have weak evidence against the null hypothesis; waiting time may have a minor effect on duration, but itâ€™s not highly convincing.\n",
    "If 0.05 â‰¥ p > 0.01: We have moderate evidence against the null hypothesis, suggesting waiting time likely impacts duration.\n",
    "If 0.01 â‰¥ p > 0.001: Strong evidence against the null hypothesis, indicating a significant linear association.\n",
    "If p â‰¤ 0.001: Very strong evidence against the null hypothesis, strongly supporting a linear relationship between waiting time and duration.\n",
    "\n",
    " Step 3: Interpret Beliefs About the Association in the Old Faithful Geyser Dataset\n",
    "Based on the p-value:\n",
    "Rejecting Hâ‚€ (if p-value is small, e.g., < 0.05): This would suggest that there is evidence for a linear association between waiting time and eruption duration. You might then believe that, on average, longer waiting times lead to longer eruptions, consistent with a predictive linear relationship.\n",
    "Failing to Reject Hâ‚€ (if p-value is large, e.g., > 0.1): This would suggest insufficient evidence for an association between waiting time and duration, meaning waiting time may not be a meaningful predictor of eruption duration.\n",
    "\n",
    "Would you like further assistance with interpreting specific output from your model or understanding the p-value threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79612c84",
   "metadata": {},
   "source": [
    "9 \n",
    "To determine if thereâ€™s evidence of a relationship between duration and wait time within only the short wait times  like 62, 64, and 66 minutes, I focus on the p-values and coefficients from each regression model. If these values indicate significance and a pattern similar to the full dataset, this supports a relationship within the short wait times as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058efe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "short_wait_limit = 62 # 64 # 66 #\n",
    "short_wait = old_faithful.waiting < short_wait_limit\n",
    "\n",
    "print(smf.ols('duration ~ waiting', data=old_faithful[short_wait]).fit().summary().tables[1])\n",
    "\n",
    "# Create a scatter plot with a linear regression trendline\n",
    "fig = px.scatter(old_faithful[short_wait], x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions for short wait times (<\"+str(short_wait_limit)+\")\", \n",
    "                 trendline='ols')\n",
    "\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae66fb3",
   "metadata": {},
   "source": [
    "1 Significance (p-value): For all three thresholds, the p-values for waiting are very low (well below 0.05), indicating a statistically significant effect of waiting time on eruption duration even within short wait times. This provides evidence against the null hypothesis, suggesting that duration and waiting are related even in the restricted dataset.\n",
    "\n",
    "2 Coefficient (waiting_coef): The coefficients for waiting across the thresholds vary slightly but remain positive (around 0.03 to 0.05), indicating that eruption duration increases with wait time. This positive correlation aligns with the relationship observed in the full dataset, suggesting a similar relationship pattern within the short wait times as well.\n",
    "\n",
    "In summary, within the context of short wait times (below thresholds like 62, 64, or 66 minutes), there is evidence in the data of a positive relationship between 'duration' and 'waiting'. This relationship mirrors the one found in the full dataset, supporting the idea that the duration of eruptions depends on the time waited, even within shorter waiting intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ddb36",
   "metadata": {},
   "source": [
    "chatbot Here's a summary of the information and findings from the previous questions and analysis:\n",
    "\n",
    "1  Objective: To investigate if there is a relationship between the duration of geyser eruptions and the waiting time between eruptions, specifically within short wait times (less than certain thresholds).\n",
    "\n",
    "2  Methodology:\n",
    "   We used a filtered dataset of geyser eruptions, focusing on \"short wait times\" defined by thresholds of 62, 64, and 66 minutes.\n",
    "   For each threshold, we ran an Ordinary Least Squares (OLS) regression with `duration` as the dependent variable and `waiting` as the independent variable.\n",
    "   We examined the p-values and coefficients from each regression to assess if `waiting` significantly impacts `duration` within these shorter intervals.\n",
    "   A scatter plot with a regression trendline was generated to visualize the relationship for each short wait threshold.\n",
    "\n",
    "3  Key Findings:\n",
    "    Significance: The p-values for `waiting` were very low in each regression (well below 0.05), indicating a statistically significant relationship between `waiting` and `duration` within short wait times. This result provides evidence against the null hypothesis (which assumed no relationship).\n",
    "    Relationship Pattern: The coefficients for `waiting` were positive (around 0.03 to 0.05) across different thresholds, suggesting that even within short wait times, longer wait times are associated with longer eruption durations.\n",
    "   Consistency with Full Dataset: The positive relationship within short wait times mirrors the trend observed in the full dataset, where longer wait times generally lead to longer eruption durations.\n",
    "\n",
    "4  Conclusion: The analysis shows that even for shorter wait times, there is evidence of a positive relationship between eruption duration and waiting time, similar to the trend in the full dataset. This indicates that the duration of geyser eruptions depends on the time waited, regardless of whether we look at the full dataset or only at shorter waiting intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20b14a",
   "metadata": {},
   "source": [
    "chatbot https://chatgpt.com/share/6722997b-407c-8007-9d08-598d241b1e13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "10 import plotly.express as px\n",
    "\n",
    "long_wait_limit = 71\n",
    "long_wait = old_faithful.waiting > long_wait_limit\n",
    "\n",
    "print(smf.ols('duration ~ waiting', data=old_faithful[long_wait]).fit().summary().tables[1])\n",
    "\n",
    "# Create a scatter plot with a linear regression trendline\n",
    "fig = px.scatter(old_faithful[long_wait], x='waiting', y='duration', \n",
    "                 title=\"Old Faithful Geyser Eruptions for short wait times (>\"+str(long_wait_limit)+\")\", \n",
    "                 trendline='ols')\n",
    "fig.show() # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define long wait filter and sample size\n",
    "long_wait_limit = 71\n",
    "long_wait_data = old_faithful[old_faithful['waiting'] > long_wait_limit]\n",
    "n_samples = 160\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# 1. Create bootstrap sampling distribution of slope coefficients\n",
    "bootstrap_slopes = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Bootstrap sample with replacement\n",
    "    sample = long_wait_data.sample(n=len(long_wait_data), replace=True)\n",
    "    # Fit linear regression model\n",
    "    model = smf.ols('duration ~ waiting', data=sample).fit()\n",
    "    # Store the slope coefficient\n",
    "    bootstrap_slopes.append(model.params['waiting'])\n",
    "\n",
    "# Convert to numpy array for easier calculation of confidence interval\n",
    "bootstrap_slopes = np.array(bootstrap_slopes)\n",
    "\n",
    "# Calculate 95% bootstrapped confidence interval\n",
    "ci_lower, ci_upper = np.percentile(bootstrap_slopes, [2.5, 97.5])\n",
    "\n",
    "# 2. Simulate null hypothesis distribution of slopes with Î²0 = 1.65, Î²1 = 0, Ïƒ = 0.37\n",
    "simulated_slopes = []\n",
    "for _ in range(n_bootstraps):\n",
    "    # Copy dataset and add simulated duration values under null hypothesis\n",
    "    simulated_data = long_wait_data.copy()\n",
    "    simulated_data['duration'] = 1.65 + 0 * simulated_data['waiting'] + np.random.normal(0, 0.37, len(simulated_data))\n",
    "    # Fit model and store slope\n",
    "    model = smf.ols('duration ~ waiting', data=simulated_data).fit()\n",
    "    simulated_slopes.append(model.params['waiting'])\n",
    "\n",
    "# 3. Report results\n",
    "print(\"95% Bootstrapped Confidence Interval for slope:\", (ci_lower, ci_upper))\n",
    "print(\"Does the interval include 0?\", \"Yes\" if ci_lower <= 0 <= ci_upper else \"No\")\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(bootstrap_slopes, bins=30, alpha=0.5, label='Bootstrap Slope Coefficients')\n",
    "plt.hist(simulated_slopes, bins=30, alpha=0.5, label='Null Hypothesis Slope Coefficients')\n",
    "plt.axvline(0, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.xlabel('Slope Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Bootstrap and Null Hypothesis Sampling Distributions of Slope Coefficients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdf7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "11 \n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display\n",
    "\n",
    "# Fit the model with the indicator variable\n",
    "model = smf.ols('duration ~ C(kind, Treatment(reference=\"short\"))', data=old_faithful)\n",
    "results = model.fit()\n",
    "display(results.summary().tables[1])  # Displays the relevant summary table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4755b",
   "metadata": {},
   "source": [
    "11.1\n",
    "1)Previous Model Specifications:\n",
    "   The initial model, 'smf.ols('duration ~ waiting', data=old_faithful)', uses waiting time as a continuous predictor, assuming a linear relationship with duration. This model provides a general slope for the impact of waiting time on duration without considering variations between â€œshortâ€ and â€œlongâ€ wait times.\n",
    "   The next two models, 'smf.ols('duration ~ waiting', data=old_faithful[short_wait])'and 'smf.ols('duration ~ waiting', data=old_faithful[long_wait])', analyze duration separately within \"short\" and \"long\" wait times, revealing potential differences within these groups but lacking direct comparability between them within the same framework.\n",
    "\n",
    "2)New Model Specification with Indicator Variable\n",
    "    The new model introduces an indicator variable for wait time categories, dividing them as â€œshortâ€ (<68 minutes) and â€œlongâ€ (â‰¥68 minutes). By using 'C(kind, Treatment(reference=\"short\"))', with \"short\" as the baseline, this model facilitates a direct comparison across the two groups. The indicator variable highlights whether the wait time falls into the â€œlongâ€ category, making it easier to examine differences in eruption duration based on wait time classification.\n",
    "\n",
    "3)Big Picture\n",
    "   This model differs from the previous ones by treating waiting time as a categorical variable, allowing for a clear comparison between â€œshortâ€ and â€œlongâ€ wait times. This specification aligns directly with the goal of determining if eruption durations vary systematically between these categories, which was less straightforward in prior continuous-only models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdec49",
   "metadata": {},
   "source": [
    "11.2 \n",
    "The null hypothesis for this model is:  ð»_0:ð›½_contrast=0\n",
    "This hypothesis states that means there is no difference in duration on average between the \"short\" and \"long\" wait time categories.\n",
    "Evidence Against the Null Hypothesis:\n",
    "after running the regression analysis with the indicator variable, examining the coefficient for C(kind, Treatment(reference=\"short\"))[T.long].\n",
    "The associated p-value for this coefficient tells us if the difference between \"long\" and \"short\" wait times is statistically significant. If the p-value is below a chosen significance level (commonly 0.05), we reject the null hypothesis, indicating that there is evidence of a significant difference in duration between \"short\" and \"long\" wait times.\n",
    "\n",
    "Results:\n",
    "If find that the p-value is below 0.05, we conclude that there is a statistically significant difference in eruption duration between \"short\" and \"long\" wait times, meaning the average duration for \"long\" wait times differs from that of \"short\" wait times. On contrast, if the p-value is above 0.05 which means fail to reject the null hypothesis, suggesting that there is no statistically significant difference in eruption duration based on the wait time category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d86c3d",
   "metadata": {},
   "source": [
    "chatbot https://chatgpt.com/share/6722a13a-0694-8007-be93-be522ffd5f2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3345f9",
   "metadata": {},
   "source": [
    "12 \n",
    "Among the four models:\n",
    "For the Model 1 It using the entire dataset with a slope and best supports the assumption of normally distributed error terms. Its histogram of residuals is roughly symmetric around zero, and the distribution aligns closely with the overlaid normal curve, indicating that the residuals follow a normal distribution reasonably well.\n",
    "\n",
    "In contrast, Model 2 (short wait data) and Model 3 (long wait data) show noticeable deviations from normality. Model 2's histogram is skewed, reflecting the limited variability in short wait times, while Model 3 shows a similar issue, likely due to analyzing only the long wait times. These skewed residuals suggest that dividing the data by wait time might not capture the underlying normal distribution of errors.\n",
    "\n",
    "Model 4 which using an indicator variable for categorized wait times has a bimodal residual distribution, indicating two distinct groups. This disrupts normality, as the residuals are not centered around a single mean, reducing model reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "13 \n",
    "A Permutation Test \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming old_faithful DataFrame is available and contains 'duration' and 'kind' columns\n",
    "n_permutations = 1000\n",
    "observed_mean_diff = old_faithful.groupby('kind')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "perm_diffs = []\n",
    "\n",
    "for _ in range(n_permutations):\n",
    "    # Shuffle 'kind' labels\n",
    "    shuffled_labels = old_faithful['kind'].sample(frac=1, replace=False).values\n",
    "    shuffled_data = old_faithful.assign(kind_shuffled=shuffled_labels)\n",
    "    # Calculate mean difference with shuffled labels\n",
    "    mean_diff = shuffled_data.groupby('kind_shuffled')['duration'].mean().iloc[::-1].diff().values[1]\n",
    "    perm_diffs.append(mean_diff)\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = np.mean([abs(diff) >= abs(observed_mean_diff) for diff in perm_diffs])\n",
    "\n",
    "print(f\"Observed Mean Difference: {observed_mean_diff}\")\n",
    "print(f\"P-value from Permutation Test: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ea4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B Bootstrap Confidence Interval\n",
    "n_bootstraps = 1000\n",
    "bootstrapped_mean_diffs = []\n",
    "\n",
    "# Bootstrapping within each group\n",
    "for _ in range(n_bootstraps):\n",
    "    short_sample = old_faithful[old_faithful['kind'] == 'short'].sample(frac=1, replace=True)\n",
    "    long_sample = old_faithful[old_faithful['kind'] == 'long'].sample(frac=1, replace=True)\n",
    "    mean_diff = long_sample['duration'].mean() - short_sample['duration'].mean()\n",
    "    bootstrapped_mean_diffs.append(mean_diff)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "conf_interval = np.quantile(bootstrapped_mean_diffs, [0.025, 0.975])\n",
    "\n",
    "print(f\"95% Bootstrap Confidence Interval for Mean Difference: {conf_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed2b23",
   "metadata": {},
   "source": [
    "14 yes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
